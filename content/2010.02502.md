---
title: 2010.02502 Denoising Diffusion Implicit Models
date: 2020-10-03
---

# [Denoising Diffusion Implicit Models](https://arxiv.org/abs/2010.02502)

Authors: Jiaming Song, Chenlin Meng, Stefano Ermon

Abstract: Denoising diffusion probabilistic models (DDPMs) have achieved high quality
image generation without adversarial training, yet they require simulating a
Markov chain for many steps to produce a sample. To accelerate sampling, we
present denoising diffusion implicit models (DDIMs), a more efficient class of
iterative implicit probabilistic models with the same training procedure as
DDPMs. In DDPMs, the generative process is defined as the reverse of a
Markovian diffusion process. We construct a class of non-Markovian diffusion
processes that lead to the same training objective, but whose reverse process
can be much faster to sample from. We empirically demonstrate that DDIMs can
produce high quality samples $10 \times$ to $50 \times$ faster in terms of
wall-clock time compared to DDPMs, allow us to trade off computation for sample
quality, and can perform semantically meaningful image interpolation directly
in the latent space.

## What, Why and How

 Here is a summary of the key points from this paper:

What:
- The paper presents denoising diffusion implicit models (DDIMs), a new class of generative models that are more efficient than denoising diffusion probabilistic models (DDPMs). 

- DDIMs are implicit probabilistic models, meaning samples are deterministically generated from latent variables rather than by simulating a stochastic process.

- DDIMs are trained using the same objectives as DDPMs, but use non-Markovian diffusion processes during training. This allows efficient sampling using reverse generative processes with fewer steps.

Why: 
- DDPMs require sampling for thousands of steps to generate high-quality samples, which is very slow. DDIMs aim to accelerate sampling from diffusion-based models.

- DDIMs can trade off sample quality for sampling speed by using shorter generative processes. This makes diffusion models more practical.

- The consistency of samples from DDIMs allows better latent space interpolation compared to DDPMs.

How:
- DDIMs are derived by generalizing the forward diffusion process used in DDPMs to non-Markovian processes. 

- This leads to the same training objectives but more flexible reverse generative processes.

- DDIMs use deterministic diffusion processes for efficient sampling with fewer steps.

- Accelerated DDIMs subsample steps from the original diffusion process length.

- DDIMs retain high-level sample features over varying generative process lengths.

In summary, DDIMs accelerate sampling for diffusion probabilistic models by using flexible non-Markovian processes and deterministic generation, achieving up to 50x speedup over DDPMs. The consistency of DDIM samples also enables better interpolation.

## Main Contributions

 Here are the key contributions of this paper on denoising diffusion implicit models (DDIMs):

- Proposes DDIMs, a new class of generative models that can efficiently produce high-quality samples through deterministic generation processes.

- Provides a theoretical framework to derive DDIMs by generalizing the forward diffusion process used in DDPMs to non-Markovian processes.

- Shows that DDIMs can be trained using the same objectives as DDPMs, allowing the use of existing models and training techniques.

- Demonstrates 10-50x speedups in sample generation compared to DDPMs with comparable or higher sample quality.

- Empirically validates properties of DDIMs:

    - Ability to trade off sample quality and sampling speed.

    - Consistency of samples for interpolation.

    - Low reconstruction error from latent encodings.

- Analyzes connections between DDIMs and neural ODEs, relating the DDIM sampling process to solving an ODE. 

- Shows equivalence between the DDIM ODE and the probability flow ODE for a variance-exploding SDE.

In summary, the key contributions are introducing DDIMs as an efficient alternative to DDPMs, providing theoretical grounding for DDIMs, and empirically demonstrating their advantages for generative modeling. The connections to ODEs are also an important contribution.

## Method Section

 Here is a summary of the method section from the paper on denoising diffusion implicit models (DDIMs):

- DDIMs are derived by generalizing the forward diffusion process used in DDPMs. The original DDPM process is Markovian. 

- The paper proposes using non-Markovian diffusion processes for training, while still matching the marginal distributions from the original process.

- This allows defining flexible reverse generative processes for efficient sampling. The generative process uses predicted denoised observations.

- The objectives for non-Markovian processes are shown to be equivalent to the DDPM objective through a theoretical analysis.

- Therefore, DDPM models can be reused for DDIMs by just changing the sampling process. No retraining is needed.

- Two types of reverse processes are introduced:

  - Stochastic DDIMs that interpolate between DDPM and deterministic sampling.
  
  - Deterministic DDIMs where samples depend only on latent codes.

- DDIMs allow accelerated sampling by using shorter generative processes that subsample steps from the full diffusion process.

- The deterministic sampling in DDIMs is analyzed as discretized Euler steps for an ODE, connecting DDIMs to neural ODEs.

- The DDIM ODE is shown to be equivalent to the probability flow ODE for a variance-exploding SDE.

In summary, the key ideas are using non-Markovian training processes to derive flexible generative sampling procedures for DDIMs that avoid costly Markov chain simulation. This enables efficient and deterministic sampling without retraining existing models.

## High-Level Pseudo Code

 Here is high-level pseudocode for the key algorithms from the denoising diffusion implicit models paper:

```python
# Training
for t in num_diffusion_steps:
  x_t = sqrt(alpha_t) * x_0 + sqrt(1 - alpha_t) * epsilon # Add noise
  
  pred_epsilon = model(x_t) # Predict noise
  
  loss = MSE(pred_epsilon, epsilon) # Denoising loss
  
opt.step(loss) # Update model
```

```python  
# Sampling - DDPM
x_T ~ N(0, I) 

for t in reversed(num_diffusion_steps):
  
  pred_x_0 = (x_t - sqrt(1-alpha_t) * model(x_t)) / sqrt(alpha_t) 
  
  x_{t-1} ~ q(x_{t-1} | x_t, pred_x_0) # Reverse process

return x_0
```

```python
# Sampling - DDIM
x_T ~ N(0, I)

for t in shortened_steps:

  pred_x_0 = (x_t - sqrt(1-alpha_t) * model(x_t)) / sqrt(alpha_t)

  x_{t-1} = pred_x_0 + f(t) * model(x_t) # Deterministic reverse 

return x_0
```

The key differences are:
- DDIM uses non-Markovian training process but matches DDPM marginals
- Sampling is accelerated by taking fewer steps
- DDIM sampling is deterministic rather than sampling from conditionals

This allows efficient sampling from diffusion models without costly Markov chain simulation.

## Detailed Pseudo Code

 Here is more detailed pseudocode to implement the key algorithms from the denoising diffusion implicit models paper:

```python
# Hyperparameters
num_diffusion_steps = 1000  
beta_schedule = np.linspace(1e-4, 0.02, num_diffusion_steps) # Annealed schedule
alphas = 1. - beta_schedule
sigmas = sqrt((1 - alphas[:-1]) / alphas[1:]) * sqrt(alphas[1:] / alphas[:-1])

# Training
for t in range(num_diffusion_steps):

  x_t = sqrt(alphas[t]) * x_0 + sqrt(1 - alphas[t]) * epsilon
  
  epsilon_pred = model(x_t, t) # Predict epsilon given x_t
  
  loss = MSE(epsilon_pred, epsilon)
  
  opt.zero_grad()
  loss.backward()
  opt.step()
  
# Sampling - DDPM

x_T = sample_gaussian(0, I) # Sample from N(0, I)

for i in reversed(range(num_diffusion_steps)):

  t = num_diffusion_steps - i - 1
  
  pred_x_0 = (x_t - sqrt(1-alphas[t]) * model(x_t)) / sqrt(alphas[t])

  mean = (sigmas[t] * pred_x_0) + sqrt(alphas[t-1] - sigmas[t]**2) * x_t  
  std = sqrt(1 - alphas[t-1]) * sigmas[t]
  x_{t-1} = sample_gaussian(mean, std) 

return x_0
```

```python
# Sampling - DDIM
x_T = sample_gaussian(0, I) 

short_steps = get_short_steps(num_diffusion_steps) # Subsample steps

for i in reversed(range(len(short_steps))):

  t = short_steps[i]  

  pred_x_0 = (x_t - sqrt(1-alphas[t]) * model(x_t)) / sqrt(alphas[t])

  x_{t-1} = pred_x_0 + sqrt(1 - alphas[t-1] - sigmas[t]**2) * model(x_t)

return x_0  
```

This shows more implementation details like the schedule, sampling from conditionals, and subsampling steps.